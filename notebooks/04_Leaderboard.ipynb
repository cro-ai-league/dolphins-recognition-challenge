{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "\n",
    "> Current leaderboard using validation set. The final leaderboard will be generated at the end of the contest using test dataset and will probably be different due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "in_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "# ignore this\n",
    "in_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from dolphins_recognition_challenge.datasets import get_dataset\n",
    "from dolphins_recognition_challenge.instance_segmentation.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "\n",
    "def parse_filename(fname):\n",
    "    tmp = fname.split(\"-\")\n",
    "    date = pd.to_datetime(tmp[1] + tmp[2] + tmp[3])\n",
    "    alias = tmp[6]\n",
    "    email = tmp[7]\n",
    "    submitted_iou = tmp[5].split(\"=\")[1]\n",
    "\n",
    "    return {\n",
    "        \"file_name\": fname,\n",
    "        \"date\": date,\n",
    "        \"alias\": alias,\n",
    "        \"email\": email,\n",
    "        \"submitted_iou\": submitted_iou,\n",
    "        \"calculated_iou\": np.nan,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "actual = parse_filename(\n",
    "    \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\"\n",
    ")\n",
    "expected = {\n",
    "    \"file_name\": \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\",\n",
    "    \"date\": pd.to_datetime(\"2020-12-22 15:35:15.513570\"),\n",
    "    \"alias\": \"dolphin123\",\n",
    "    \"email\": \"name.surname@gmail.com\",\n",
    "    \"submitted_iou\": \"0.46613\",\n",
    "    \"calculated_iou\": np.nan,\n",
    "}\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "my_bucket = s3.Bucket(\"ai-league.cisex.org\")\n",
    "private_leaderboard_path = Path(\"private_leaderboard.csv\")\n",
    "public_leaderboard_path = Path(\"leaderboard.csv\")\n",
    "\n",
    "\n",
    "def get_submissions_from_s3(private_leaderboard_path=private_leaderboard_path):\n",
    "    \"\"\"Downloads the zip file from s3 if there is no record of it in the csv file\"\"\"\n",
    "    if private_leaderboard_path.exists():\n",
    "        private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    else:\n",
    "        private_leaderboard = dict(file_name=[])\n",
    "\n",
    "    # download file into models_for_evaluation directory\n",
    "    s3_objects = [\n",
    "        s3_object\n",
    "        for s3_object in my_bucket.objects.all()\n",
    "        if Path(s3_object.key).match(\"*submission*.zip\")\n",
    "        and Path(s3_object.key).name not in list(private_leaderboard[\"file_name\"])\n",
    "    ]\n",
    "    if len(s3_objects) > 0:\n",
    "        for i, s3_object in enumerate(s3_objects):\n",
    "            print(f\"Downloading {i+1}/{len(s3_objects)} from S3...\")\n",
    "            my_bucket.download_file(s3_object.key, f\"models_for_evaluation/{Path(s3_object.key).name}\")\n",
    "\n",
    "        # return new entries\n",
    "        new_entries = pd.Series([Path(s3_object.key).name for s3_object in s3_objects]).apply(parse_filename).apply(pd.Series)\n",
    "    else:\n",
    "        x = \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\"\n",
    "        new_entries = pd.Series([x]).apply(parse_filename).apply(pd.Series).iloc[:0, :]\n",
    "        \n",
    "    return new_entries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def public(private_leaderboard):\n",
    "    return private_leaderboard[[\"alias\", \"date\", \"submitted_iou\", \"calculated_iou\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1/1 from S3...\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    new_entries = get_submissions_from_s3()\n",
    "    public(new_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "def merge_with_private_leaderboard(\n",
    "    new_entries, private_leaderboard_path=private_leaderboard_path\n",
    "):\n",
    "    # merge private leaderboard and new_entries if needed\n",
    "    new_entries[\"calculated_iou\"] = np.nan\n",
    "    if private_leaderboard_path.exists():\n",
    "        private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "        private_leaderboard = pd.concat([private_leaderboard, new_entries], axis=0)\n",
    "        private_leaderboard = private_leaderboard.drop_duplicates(subset=\"file_name\")\n",
    "    else:\n",
    "        private_leaderboard = new_entries\n",
    "\n",
    "    private_leaderboard.to_csv(private_leaderboard_path, index=False)\n",
    "\n",
    "    return private_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    private_leaderboard = merge_with_private_leaderboard(new_entries)\n",
    "    public(private_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def evaluate_model(model_path) -> float:\n",
    "    # do it\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        with zipfile.ZipFile(model_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(path=d)\n",
    "            unzipped_path = [x for x in Path(d).glob(\"submiss*\")][0]\n",
    "\n",
    "        model = torch.load(unzipped_path / \"model.pt\")\n",
    "        data_loader, data_loader_test = get_dataset(\"segmentation\", batch_size=4)\n",
    "        iou, iou_df = iou_metric(model, data_loader_test.dataset)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    actual = evaluate_model(\n",
    "        Path(\n",
    "            \"models_for_evaluation/uploaded-2021-01-05T15:01:23.563795-submission-iou=0.44003-dolphin123-name.surname@gmail.com-2021-01-05T15:01:21.655750.zip\"\n",
    "        )\n",
    "    )\n",
    "    expected = 0.44003\n",
    "    np.testing.assert_almost_equal(actual, expected, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def evaluate_private_leaderboard(private_leaderboard_path=private_leaderboard_path):\n",
    "    private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    new_entries = private_leaderboard.loc[private_leaderboard[\"calculated_iou\"].isna()]\n",
    "    \n",
    "    n = new_entries.shape[0]\n",
    "    for i, ix in enumerate(new_entries.index):\n",
    "        row = new_entries.loc[ix]\n",
    "        file_name, alias, dt = row[\"file_name\"], row[\"alias\"], row[\"date\"]\n",
    "        print(f\"Evaluating model {i+1}/{n} for {alias} submitted at {dt}...\")\n",
    "        calculated_iou = evaluate_model(f\"models_for_evaluation/{file_name}\")\n",
    "        private_leaderboard.loc[ix, \"calculated_iou\"] = calculated_iou\n",
    "        \n",
    "    private_leaderboard.to_csv(private_leaderboard_path, index=False)\n",
    "    return private_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    private_leaderboard = pd.read_csv(\"private_leaderboard.csv\")\n",
    "    assert np.nan not in list(private_leaderboard[\"calculated_iou\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def save_public_leaderboard(private_leaderboard_path=private_leaderboard_path, public_leaderboard_path=public_leaderboard_path):\n",
    "    private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    public_leaderboard = public(private_leaderboard)\n",
    "    public_leaderboard.to_csv(public_leaderboard_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    save_public_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_leaderboard(public_leaderboard_path=public_leaderboard_path):\n",
    "    public_leaderboard = pd.read_csv(public_leaderboard_path)\n",
    "    public_leaderboard = public_leaderboard[(public_leaderboard.alias != \"dolphin123\") & (public_leaderboard.alias != \"malimedo\")]\n",
    "    public_leaderboard = public_leaderboard.sort_values(by=[\"calculated_iou\"], ascending=False).reset_index(drop=True) \n",
    "    public_leaderboard.index = public_leaderboard.index + 1\n",
    "    return public_leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary leaderboard calculated daily using validation data. The final leaderboard will be calculated using test dataset unavailable to participants and will most likely be different than the one provided here due to overfitting on the validation dataset. Please see the following link for the details: https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>date</th>\n",
       "      <th>submitted_iou</th>\n",
       "      <th>calculated_iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prvi_pokušaj</td>\n",
       "      <td>2021-01-13 14:29:59.372504</td>\n",
       "      <td>0.44925</td>\n",
       "      <td>0.449246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alias                        date  submitted_iou  calculated_iou\n",
       "1  prvi_pokušaj  2021-01-13 14:29:59.372504        0.44925        0.449246"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "if not in_test:\n",
    "    get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
