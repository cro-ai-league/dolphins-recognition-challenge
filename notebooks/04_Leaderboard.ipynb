{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaderboard\n",
    "\n",
    "> Current leaderboard using validation set. The final leaderboard will be generated at the end of the contest using test dataset and will probably be different due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "in_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip\n",
    "# ignore this\n",
    "in_test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import torch\n",
    "import tempfile\n",
    "\n",
    "from dolphins_recognition_challenge.datasets import get_dataset\n",
    "from dolphins_recognition_challenge.instance_segmentation.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "\n",
    "def parse_filename(fname):\n",
    "    tmp = fname.split(\"-\")\n",
    "    date = pd.to_datetime(tmp[1] + tmp[2] + tmp[3])\n",
    "    alias = tmp[6]\n",
    "    email = tmp[7]\n",
    "    submitted_iou = tmp[5].split(\"=\")[1]\n",
    "\n",
    "    return {\n",
    "        \"file_name\": fname,\n",
    "        \"date\": date,\n",
    "        \"alias\": alias,\n",
    "        \"email\": email,\n",
    "        \"submitted_iou\": submitted_iou,\n",
    "        \"calculated_iou\": np.nan,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "actual = parse_filename(\n",
    "    \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\"\n",
    ")\n",
    "expected = {\n",
    "    \"file_name\": \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\",\n",
    "    \"date\": pd.to_datetime(\"2020-12-22 15:35:15.513570\"),\n",
    "    \"alias\": \"dolphin123\",\n",
    "    \"email\": \"name.surname@gmail.com\",\n",
    "    \"submitted_iou\": \"0.46613\",\n",
    "    \"calculated_iou\": np.nan,\n",
    "}\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "my_bucket = s3.Bucket(\"ai-league.cisex.org\")\n",
    "private_leaderboard_path = Path(\"private_leaderboard.csv\")\n",
    "public_leaderboard_path = Path(\"leaderboard.csv\")\n",
    "\n",
    "\n",
    "def get_submissions_from_s3(private_leaderboard_path=private_leaderboard_path):\n",
    "    \"\"\"Downloads the zip file from s3 if there is no record of it in the csv file\"\"\"\n",
    "    if private_leaderboard_path.exists():\n",
    "        private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    else:\n",
    "        private_leaderboard = dict(file_name=[])\n",
    "\n",
    "    # download file into models_for_evaluation directory\n",
    "    s3_objects = [\n",
    "        s3_object\n",
    "        for s3_object in my_bucket.objects.all()\n",
    "        if Path(s3_object.key).match(\"*submission*.zip\")\n",
    "        and Path(s3_object.key).name not in list(private_leaderboard[\"file_name\"])\n",
    "    ]\n",
    "    if len(s3_objects) > 0:\n",
    "        for i, s3_object in enumerate(s3_objects):\n",
    "            print(f\"Downloading {i+1}/{len(s3_objects)} from S3...\")\n",
    "            my_bucket.download_file(s3_object.key, f\"models_for_evaluation/{Path(s3_object.key).name}\")\n",
    "\n",
    "        # return new entries\n",
    "        new_entries = pd.Series([Path(s3_object.key).name for s3_object in s3_objects]).apply(parse_filename).apply(pd.Series)\n",
    "    else:\n",
    "        x = \"uploaded-2020-12-22T15:35:15.513570-submission-iou=0.46613-dolphin123-name.surname@gmail.com-2020-12-22T15:35:04.875962.zip\"\n",
    "        new_entries = pd.Series([x]).apply(parse_filename).apply(pd.Series).iloc[:0, :]\n",
    "        \n",
    "    return new_entries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def public(private_leaderboard):\n",
    "    return private_leaderboard[[\"alias\", \"date\", \"submitted_iou\", \"calculated_iou\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1/34 from S3...\n",
      "Downloading 2/34 from S3...\n",
      "Downloading 3/34 from S3...\n",
      "Downloading 4/34 from S3...\n",
      "Downloading 5/34 from S3...\n",
      "Downloading 6/34 from S3...\n",
      "Downloading 7/34 from S3...\n",
      "Downloading 8/34 from S3...\n",
      "Downloading 9/34 from S3...\n",
      "Downloading 10/34 from S3...\n",
      "Downloading 11/34 from S3...\n",
      "Downloading 12/34 from S3...\n",
      "Downloading 13/34 from S3...\n",
      "Downloading 14/34 from S3...\n",
      "Downloading 15/34 from S3...\n",
      "Downloading 16/34 from S3...\n",
      "Downloading 17/34 from S3...\n",
      "Downloading 18/34 from S3...\n",
      "Downloading 19/34 from S3...\n",
      "Downloading 20/34 from S3...\n",
      "Downloading 21/34 from S3...\n",
      "Downloading 22/34 from S3...\n",
      "Downloading 23/34 from S3...\n",
      "Downloading 24/34 from S3...\n",
      "Downloading 25/34 from S3...\n",
      "Downloading 26/34 from S3...\n",
      "Downloading 27/34 from S3...\n",
      "Downloading 28/34 from S3...\n",
      "Downloading 29/34 from S3...\n",
      "Downloading 30/34 from S3...\n",
      "Downloading 31/34 from S3...\n",
      "Downloading 32/34 from S3...\n",
      "Downloading 33/34 from S3...\n",
      "Downloading 34/34 from S3...\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    new_entries = get_submissions_from_s3()\n",
    "    public(new_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporti\n",
    "\n",
    "def merge_with_private_leaderboard(\n",
    "    new_entries, private_leaderboard_path=private_leaderboard_path\n",
    "):\n",
    "    # merge private leaderboard and new_entries if needed\n",
    "    new_entries[\"calculated_iou\"] = np.nan\n",
    "    if private_leaderboard_path.exists():\n",
    "        private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "        private_leaderboard = pd.concat([private_leaderboard, new_entries], axis=0)\n",
    "        private_leaderboard = private_leaderboard.drop_duplicates(subset=\"file_name\")\n",
    "    else:\n",
    "        private_leaderboard = new_entries\n",
    "\n",
    "    private_leaderboard.to_csv(private_leaderboard_path, index=False)\n",
    "\n",
    "    return private_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    private_leaderboard = merge_with_private_leaderboard(new_entries)\n",
    "    public(private_leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def evaluate_model(model_path) -> float:\n",
    "    # do it\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        with zipfile.ZipFile(model_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(path=d)\n",
    "            unzipped_path = [x for x in Path(d).glob(\"submiss*\")][0]\n",
    "\n",
    "        model = torch.load(unzipped_path / \"model.pt\")\n",
    "        data_loader, data_loader_test = get_dataset(\"segmentation\", batch_size=4)\n",
    "        iou, iou_df = iou_metric(model, data_loader_test.dataset)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    actual = evaluate_model(\n",
    "        Path(\n",
    "            \"models_for_evaluation/uploaded-2021-01-05T15:01:23.563795-submission-iou=0.44003-dolphin123-name.surname@gmail.com-2021-01-05T15:01:21.655750.zip\"\n",
    "        )\n",
    "    )\n",
    "    expected = 0.44003\n",
    "    np.testing.assert_almost_equal(actual, expected, decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def evaluate_private_leaderboard(private_leaderboard_path=private_leaderboard_path):\n",
    "    private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    new_entries = private_leaderboard.loc[private_leaderboard[\"calculated_iou\"].isna()]\n",
    "    \n",
    "    n = new_entries.shape[0]\n",
    "    for i, ix in enumerate(new_entries.index):\n",
    "        row = new_entries.loc[ix]\n",
    "        file_name, alias, dt = row[\"file_name\"], row[\"alias\"], row[\"date\"]\n",
    "        print(f\"Evaluating model {i+1}/{n} for {alias} submitted at {dt}...\")\n",
    "        calculated_iou = evaluate_model(f\"models_for_evaluation/{file_name}\")\n",
    "        private_leaderboard.loc[ix, \"calculated_iou\"] = calculated_iou\n",
    "        \n",
    "    private_leaderboard.to_csv(private_leaderboard_path, index=False)\n",
    "    return private_leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    private_leaderboard = pd.read_csv(\"private_leaderboard.csv\")\n",
    "    assert np.nan not in list(private_leaderboard[\"calculated_iou\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\n",
    "def save_public_leaderboard(private_leaderboard_path=private_leaderboard_path, public_leaderboard_path=public_leaderboard_path):\n",
    "    private_leaderboard = pd.read_csv(private_leaderboard_path)\n",
    "    public_leaderboard = public(private_leaderboard)\n",
    "    public_leaderboard.to_csv(public_leaderboard_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\n",
    "if not in_test:\n",
    "    save_public_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "\n",
    "def get_leaderboard(public_leaderboard_path=public_leaderboard_path):\n",
    "    public_leaderboard = pd.read_csv(public_leaderboard_path)\n",
    "    public_leaderboard = public_leaderboard[\n",
    "        (public_leaderboard.alias != \"dolphin123\")\n",
    "        & (public_leaderboard.alias != \"malimedo\")\n",
    "        & (public_leaderboard.alias != \"prvi_pokušaj\")\n",
    "    ]\n",
    "    public_leaderboard = public_leaderboard.sort_values(\n",
    "        by=[\"submitted_iou\"], ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    public_leaderboard[\"alias_groupby\"] = public_leaderboard[\"alias\"]\n",
    "    public_leaderboard = (\n",
    "        public_leaderboard.groupby([\"alias_groupby\"], sort=False)\n",
    "        .max()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    public_leaderboard.index = public_leaderboard.index + 1\n",
    "    return public_leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a temporary leaderboard calculated daily using validation data. The final leaderboard will be calculated using test dataset unavailable to participants and will most likely be different than the one provided here due to overfitting on the validation dataset. Please see the following link for the details: https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alias</th>\n",
       "      <th>date</th>\n",
       "      <th>submitted_iou</th>\n",
       "      <th>calculated_iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tekashi</td>\n",
       "      <td>2021-04-01 19:39:29.095649</td>\n",
       "      <td>0.50548</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dolphinSantiago</td>\n",
       "      <td>2021-03-28 21:13:52.740719</td>\n",
       "      <td>0.48156</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dupin</td>\n",
       "      <td>2021-03-31 17:54:29.801730</td>\n",
       "      <td>0.46228</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orka</td>\n",
       "      <td>2021-04-02 01:15:54.079218</td>\n",
       "      <td>0.45201</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dupincek</td>\n",
       "      <td>2021-03-29 17:04:06.266327</td>\n",
       "      <td>0.44912</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Boto</td>\n",
       "      <td>2021-03-31 22:54:40.595595</td>\n",
       "      <td>0.44678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stokic</td>\n",
       "      <td>2021-02-21 18:51:53.232539</td>\n",
       "      <td>0.43552</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>firstML</td>\n",
       "      <td>2021-02-28 09:55:59.155951</td>\n",
       "      <td>0.42529</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rangoiv_0.0</td>\n",
       "      <td>2021-03-11 21:18:19.755089</td>\n",
       "      <td>0.41769</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>testy</td>\n",
       "      <td>2021-03-12 15:06:14.210987</td>\n",
       "      <td>0.39001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              alias                        date  submitted_iou  calculated_iou\n",
       "1           tekashi  2021-04-01 19:39:29.095649        0.50548             NaN\n",
       "2   dolphinSantiago  2021-03-28 21:13:52.740719        0.48156             NaN\n",
       "3             Dupin  2021-03-31 17:54:29.801730        0.46228             NaN\n",
       "4              Orka  2021-04-02 01:15:54.079218        0.45201             NaN\n",
       "5          dupincek  2021-03-29 17:04:06.266327        0.44912             NaN\n",
       "6              Boto  2021-03-31 22:54:40.595595        0.44678             NaN\n",
       "7            stokic  2021-02-21 18:51:53.232539        0.43552             NaN\n",
       "8           firstML  2021-02-28 09:55:59.155951        0.42529             NaN\n",
       "9       rangoiv_0.0  2021-03-11 21:18:19.755089        0.41769             NaN\n",
       "10            testy  2021-03-12 15:06:14.210987        0.39001             NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_input\n",
    "\n",
    "if not in_test:\n",
    "    display(get_leaderboard())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('leaderboard.csv')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_leaderboard_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
